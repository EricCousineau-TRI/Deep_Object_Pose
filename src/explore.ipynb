{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "from os.path import dirname, realpath, join\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "import yaml\n",
    "import pandas as pd\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(join(cur, \"training\"))\n",
    "sys.path.append(join(cur))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train import MultipleVertexJson\n",
    "from validate import Comparison, Pose, load_model_cm, get_mesh_file, add_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"003_cracker_box\"\n",
    "model_cm = load_model_cm(get_mesh_file(model))\n",
    "def pose_error(X_est, X_gt):\n",
    "    return add_metric(X_est.R, X_est.t_cm, X_gt.R, X_gt.t_cm, model_cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cPickle as pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../comp_list.pkl\") as f:\n",
    "    comp_list = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(0, dtype=Comparison.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.recarray([acc, acc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc.view(np.recarray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ".sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = np.array(0, dtype=Comparison.dtype).view(np.recarray)\n",
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.recarray([acc, acc]).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(comp_list[0].compute_metrics(pose_error, error_threshold))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute(error_threshold, skip_no_gt=True):\n",
    "    metrics = []\n",
    "    for i, comp in enumerate(comp_list):\n",
    "        if comp.num_gt == 0 and skip_no_gt:\n",
    "            continue\n",
    "        metrics.append(comp.compute_metrics(pose_error, error_threshold))\n",
    "    metrics = np.hstack(metrics)\n",
    "    return pd.DataFrame(metrics).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ets = []\n",
    "metrics = []\n",
    "for error_threshold in tqdm_notebook(np.linspace(0, 100, 100)):\n",
    "    ets.append(error_threshold / 100)\n",
    "    metrics.append(compute(error_threshold))\n",
    "metrics = pd.DataFrame(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(ets, metrics[\"num_tp\"])\n",
    "plt.xlabel(\"ADD Metric (m)\")\n",
    "plt.ylabel(\"F1 Score\")\n",
    "plt.title(model)\n",
    "plt.savefig(\"/home/eacousineau/proj/tri/proj/perception/plots/simple_stuff.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
